from trl import RewardConfig, RewardTrainer
from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
import torch

# âœ… Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-SFT/checkpoint-1975")

# âœ… Load QLoRA 4-bit Quantization Config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,  # âœ… Better stability than fp16
    bnb_4bit_use_double_quant=True,
)

# âœ… Load Model with Quantization
model = AutoModelForSequenceClassification.from_pretrained(
    "Qwen/Qwen2.5-0.5B-SFT/checkpoint-1975",
    num_labels=1,
    quantization_config=bnb_config,  
    device_map="auto",
)

# âœ… Ensure Pad Token is Correct
model.config.pad_token_id = tokenizer.pad_token_id

# âœ… Apply LoRA for Efficient Fine-Tuning
lora_config = LoraConfig(
    r=16,  # LoRA rank
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="SEQ_CLS",  # âœ… Required for reward models
)

# âœ… Convert to LoRA model
model = get_peft_model(model, lora_config)

# âœ… Ensure Training Mode
model.train()

# âœ… Disable `use_cache` for Gradient Checkpointing
model.config.use_cache = False

# âœ… Load Dataset (Use a Subset to Reduce Training Time)
dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train[:50%]")  # âœ… Use only 20% for speed

# âœ… Optimized Training Parameters
training_args = RewardConfig(
    output_dir="Qwen2.5-0.5B-Reward",
    per_device_train_batch_size=4,  # âœ… Small batch size for memory efficiency
    gradient_accumulation_steps=8,  # âœ… Reduces memory use
    num_train_epochs=1,  # âœ… Less training time
    gradient_checkpointing=True,  # âœ… Saves memory
    bf16=True,  # âœ… More stable than fp16
    optim="paged_adamw_32bit",
    save_strategy="epoch",
    report_to="none"
)

#    attn_implementation="flash_attention_2",  # âœ… Optional, speeds up training (~20%)

# âœ… Initialize Trainer
trainer = RewardTrainer(
    args=training_args,
    model=model,
    processing_class=tokenizer,
    train_dataset=dataset,
)

# âœ… Debug: Check Tensor Types
inputs = tokenizer("Test input", return_tensors="pt")
inputs["labels"] = torch.tensor([1.0]).to("cuda")  # âœ… Reward model needs float labels
inputs = {key: value.to("cuda") for key, value in inputs.items()}

# âœ… Forward Pass Check
outputs = model(**inputs)
loss = outputs.get("loss", None)

if loss is None:
    raise RuntimeError("ðŸš¨ Loss is None! Ensure labels are correctly formatted.")
elif not loss.requires_grad:
    raise RuntimeError("ðŸš¨ Loss does not have gradients! Check LoRA setup.")

# âœ… Start Training
trainer.train()
